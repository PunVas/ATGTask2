# Multimodal RAG App

A Streamlit app that lets you upload a PDF and ask questions about its content — text *and* images — using a multimodal RAG (Retrieval-Augmented Generation) setup. It uses Ollama (with LLaVA) behind the scenes to do the smart answering.

---

##What It Does

* **Upload a PDF** — and we’ll extract both the text and the images from it.
* **Generate embeddings** — text via `SentenceTransformer`, images via `CLIP`.
* **Search relevant chunks** — using FAISS for quick similarity search.
* **Query Ollama** — sends selected text and image data to an Ollama-powered LLaVA model to answer your question.
* **Keep a chat history** — so you don’t lose track of previous Q\&As.
* **Feedback loop** — thumbs up/down for each response, with an option to regenerate if needed.
* **See document stats** — get a quick peek at how many text chunks and images were found.

---

## Prerequisites

Before you run the app, make sure you have the following ready:

* **Python 3.7 or later**
* **Ollama** installed from [https://ollama.com](https://ollama.com)
* **Multimodal model** for Ollama (like `llava`) pulled locally
* A few Python packages (see install step)

---

## Setup

1. **Save the app**
   Drop the Python code into a file, e.g., `app.py`.

2. **Install dependencies**
   Run this in your terminal:

   ```bash
   pip install streamlit PyMuPDF Pillow numpy faiss-cpu sentence-transformers transformers requests torch
   ```
   
3. **Start Ollama**
   Make sure Ollama is running in the background:

   ```bash
   ollama run llava
   ```

4. **Pull the LLaVA model**
   If you haven't done this yet:

   ```bash
   ollama pull llava
   ```

---

## How to Run the App

From your terminal:

```bash
streamlit run app.py
```

It should pop open in your browser automatically.

---

## How to Use It

1. **Upload a PDF** from the sidebar.
2. **Click "Process"** — give it a second to extract everything.
3. **Ask your question** (any question related to the document).
4. **Click "Submit"** — and the model will give you an answer with supporting image context (if available).
5. **Review the answer**, check what images were used, and leave feedback.
6. **Regenerate** if needed (especially if something feels off).
7. **Browse previous Q\&As** in the chat history area.
8. **Open the “Document Info” panel** to view image/text chunk stats.

---

## Notes on Ollama

This app expects your Ollama server to be accessible at:

```
http://localhost:11434
```

And it needs a model that can handle image input — like `llava`. If you're using something else, make sure it supports multimodal inputs.

---

Let me know if you'd like a version tailored for deployment (e.g., on Hugging Face Spaces or Docker)!
